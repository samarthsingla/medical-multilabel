{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8107454,"sourceType":"datasetVersion","datasetId":4788727},{"sourceId":31703,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":26588}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n#set device to cuda, if not available check mps else use cpu\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n        print(\"MPS not available because the current PyTorch install was not \"\n              \"built with MPS enabled.\")\n    else:\n        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n              \"and/or you do not have an MPS-enabled device on this machine.\")\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\nelse:\n    device = torch.device('mps')\n\n    \n#print device type\nprint(\"Current Device\", device)\ntorch.manual_seed(0)\nKAGGLE = 1\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:11:15.743853Z","iopub.execute_input":"2024-04-15T17:11:15.744568Z","iopub.status.idle":"2024-04-15T17:11:20.278296Z","shell.execute_reply.started":"2024-04-15T17:11:15.744525Z","shell.execute_reply":"2024-04-15T17:11:20.277263Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"MPS not available because the current PyTorch install was not built with MPS enabled.\nCurrent Device cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Summary of Different Model Configurations:\nGeneral: BERT_Base (Both cased and uncased) is used as the base pre-trained model for all configurations. \nBatch Size is 32 unless otherwise specified.\nlr is 1e-4 unless otherwise specified. \n\nNote: \n- In all configurations, the classification head (added on top of the CLS token embedding from BERT) is unfrozen\n- I have experimented with cased vs uncased BERT, various levels of unfreezing of layers and even gradual unfreezing with adjusted learning rate (lowered to 1e-5)\n- Accuracy refers to the number of individual labels correctly predicted/total predictions (14 \\* num_examples)\n\n## Experimentation\n### Frozen Parameters\nFirst I explored cased vs uncased models by freezing the bert model entirely. \nI concluded that cased vs uncased did not make much difference in accuracy but did make a 5-8 minute difference (per epoch) in training time. So I chose uncased (less training time).\n\n### One Encoder Layer Unfrozen (Adjusted Learning Rate)\nI trained the model for 2 epochs in this case:\n- Epoch 1: Loss = 0.43, Accuracy = 83.8\n- Epoch 2: Loss = 0.37, Accuracy = 84.4, lr = 2e-5\n\nThis improved accuracy so over frozen so I decided to unfreeze some encoder layers. \n\n## Final Configurations \\[Embeddings Finetuned\\]\nAfter for cased vs uncased, level of unfreezing beneficial I decided the final \ntraining type: *Encoder Layer(s) and Embeddings Unfrozen*\nUNCASED, Batch size = 16\n\n- Epoch 1: Loss = 0.2703, Accuracy = 87.75, 1 layer unfrozen\n- Epoch 2: lr = 4e-15, 2 layers unfrozen \\[Did not save Accuracy data\\]\n- (Final) Epoch 3: Loss =  0.2167, Accuracy = 88., lr = 2e-15, 2 layers unfrozen\n\nFinal Test Accuracy: 89.5%\nFinal Validation Accuracy: 89.8%\n\nThe aggregate metrics can be found below:\n```\nTEST DATA\nLabel: A | Precision: 0.8407833120476799 | Recall: 0.836155800169348 | F1-Score: 0.8384631713012098\nLabel: B | Precision: 0.9690526315789474 | Recall: 0.9911714039621017 | F1-Score: 0.9799872258888653\nLabel: C | Precision: 0.8568953568953569 | Recall: 0.9435330026707364 | F1-Score: 0.8981296531686943\nLabel: D | Precision: 0.9467146126185028 | Recall: 0.9243536546441111 | F1-Score: 0.9354005167958656\nLabel: E | Precision: 0.8542356838618078 | Recall: 0.9228016359918201 | F1-Score: 0.8871958712214303\nLabel: F | Precision: 0.8127250900360145 | Recall: 0.8166465621230398 | F1-Score: 0.8146811070998796\nLabel: G | Precision: 0.8505970563732297 | Recall: 0.9043401240035429 | F1-Score: 0.8766456783056669\nLabel: H | Precision: 0.6428571428571429 | Recall: 0.11707317073170732 | F1-Score: 0.19807427785419532\nLabel: I | Precision: 0.753393665158371 | Recall: 0.6294896030245747 | F1-Score: 0.685890834191555\nLabel: J | Precision: 0.7595628415300546 | Recall: 0.556 | F1-Score: 0.6420323325635104\nLabel: L | Precision: 0.8072289156626506 | Recall: 0.44966442953020136 | F1-Score: 0.5775862068965518\nLabel: M | Precision: 0.8943694741740345 | Recall: 0.9218225419664269 | F1-Score: 0.9078885214926783\nLabel: N | Precision: 0.834759117514633 | Recall: 0.8362652232746955 | F1-Score: 0.8355114916629112\nLabel: Z | Precision: 0.8485294117647059 | Recall: 0.7513020833333334 | F1-Score: 0.7969613259668509\nMacro Precision: 0.8336931651480807 | Macro Recall: 0.7571870882446886 | Macro F1-Score: 0.7767463010292762\nMicro Precision: 0.8782085513902239 | Micro Recall: 0.8702155430909796 | Micro F1-Score: 0.8741937770217592\n\n\nVALIDATION DATA:\nLabel: A | Precision: 0.8407833120476799 | Recall: 0.836155800169348 | F1-Score: 0.8384631713012098\nLabel: B | Precision: 0.9690526315789474 | Recall: 0.9911714039621017 | F1-Score: 0.9799872258888653\nLabel: C | Precision: 0.8568953568953569 | Recall: 0.9435330026707364 | F1-Score: 0.8981296531686943\nLabel: D | Precision: 0.9467146126185028 | Recall: 0.9243536546441111 | F1-Score: 0.9354005167958656\nLabel: E | Precision: 0.8542356838618078 | Recall: 0.9228016359918201 | F1-Score: 0.8871958712214303\nLabel: F | Precision: 0.8127250900360145 | Recall: 0.8166465621230398 | F1-Score: 0.8146811070998796\nLabel: G | Precision: 0.8505970563732297 | Recall: 0.9043401240035429 | F1-Score: 0.8766456783056669\nLabel: H | Precision: 0.6428571428571429 | Recall: 0.11707317073170732 | F1-Score: 0.19807427785419532\nLabel: I | Precision: 0.753393665158371 | Recall: 0.6294896030245747 | F1-Score: 0.685890834191555\nLabel: J | Precision: 0.7595628415300546 | Recall: 0.556 | F1-Score: 0.6420323325635104\nLabel: L | Precision: 0.8072289156626506 | Recall: 0.44966442953020136 | F1-Score: 0.5775862068965518\nLabel: M | Precision: 0.8943694741740345 | Recall: 0.9218225419664269 | F1-Score: 0.9078885214926783\nLabel: N | Precision: 0.834759117514633 | Recall: 0.8362652232746955 | F1-Score: 0.8355114916629112\nLabel: Z | Precision: 0.8485294117647059 | Recall: 0.7513020833333334 | F1-Score: 0.7969613259668509\nMacro Precision: 0.8336931651480807 | Macro Recall: 0.7571870882446886 | Macro F1-Score: 0.7767463010292762\nMicro Precision: 0.8782085513902239 | Micro Recall: 0.8702155430909796 | Micro F1-Score: 0.8741937770217592\n```\n### Adding Dropout Layer\nAs a final experiment, I added dropout layers in the classification head and shortened it.\n\nResults are as follows:\n```\nTESTING DATA:\nLabel: A | Precision: 0.8210930828351836 | Recall: 0.8207426376440461 | F1-Score: 0.8209178228388474\nLabel: B | Precision: 0.9713916971391697 | Recall: 0.9764324324324324 | F1-Score: 0.9739055423765366\nLabel: C | Precision: 0.8890631125049 | Recall: 0.8692985818321196 | F1-Score: 0.8790697674418604\nLabel: D | Precision: 0.9225642653125992 | Recall: 0.9314322332585709 | F1-Score: 0.9269770408163265\nLabel: E | Precision: 0.8126901347240331 | Recall: 0.9572562068082928 | F1-Score: 0.8790692208250087\nLabel: F | Precision: 0.8943089430894309 | Recall: 0.6307339449541285 | F1-Score: 0.7397444519166106\nLabel: G | Precision: 0.8169051404345522 | Recall: 0.9099763872491146 | F1-Score: 0.8609327003630272\nLabel: H | Precision: 0.5253164556962026 | Recall: 0.13651315789473684 | F1-Score: 0.21671018276762402\nLabel: I | Precision: 0.7130620985010707 | Recall: 0.6098901098901099 | F1-Score: 0.6574531095755183\nLabel: J | Precision: 0.7045454545454546 | Recall: 0.5646630236794171 | F1-Score: 0.6268958543983822\nLabel: L | Precision: 0.6530303030303031 | Recall: 0.5693527080581242 | F1-Score: 0.6083274523641496\nLabel: M | Precision: 0.8991470145509283 | Recall: 0.847682119205298 | F1-Score: 0.8726564402240078\nLabel: N | Precision: 0.8277010947168015 | Recall: 0.7763392857142857 | F1-Score: 0.8011978806726561\nLabel: Z | Precision: 0.8527272727272728 | Recall: 0.6051612903225806 | F1-Score: 0.7079245283018867\nMacro Precision: 0.8073961478434216 | Macro Recall: 0.7289624370673756 | Macro F1-Score: 0.7551272853487457\nMicro Precision: 0.8606566142658539 | Micro Recall: 0.8485274478105012 | Micro F1-Score: 0.8545489939299555\nTest Accuracy 0.8825857142857143\n\n\nVALIDATION DATA\nLabel: A | Precision: 0.8199152542372882 | Recall: 0.8255119453924915 | F1-Score: 0.8227040816326532\nLabel: B | Precision: 0.9751712328767124 | Recall: 0.9735042735042735 | F1-Score: 0.9743370402053038\nLabel: C | Precision: 0.9089093088294047 | Recall: 0.8627227910504361 | F1-Score: 0.8852140077821012\nLabel: D | Precision: 0.9135802469135802 | Recall: 0.925 | F1-Score: 0.9192546583850932\nLabel: E | Precision: 0.817765168048887 | Recall: 0.9563552833078101 | F1-Score: 0.8816470588235295\nLabel: F | Precision: 0.8991869918699187 | Recall: 0.6415313225058005 | F1-Score: 0.7488151658767772\nLabel: G | Precision: 0.8150470219435737 | Recall: 0.9217134416543574 | F1-Score: 0.8651046721197837\nLabel: H | Precision: 0.5297619047619048 | Recall: 0.15724381625441697 | F1-Score: 0.24250681198910082\nLabel: I | Precision: 0.7215777262180975 | Recall: 0.5791433891992551 | F1-Score: 0.6425619834710744\nLabel: J | Precision: 0.703962703962704 | Recall: 0.5666041275797373 | F1-Score: 0.6278586278586279\nLabel: L | Precision: 0.65625 | Recall: 0.5976714100905562 | F1-Score: 0.6255924170616113\nLabel: M | Precision: 0.8966550174737893 | Recall: 0.8499763369616659 | F1-Score: 0.8726919339164237\nLabel: N | Precision: 0.8230297310051912 | Recall: 0.7625710537822474 | F1-Score: 0.7916477530640037\nLabel: Z | Precision: 0.8250428816466552 | Recall: 0.6222509702457956 | F1-Score: 0.7094395280235988\nMacro Precision: 0.8075610849848361 | Macro Recall: 0.7315571543949175 | Macro F1-Score: 0.7578125528721201\nMicro Precision: 0.8615215229436 | Micro Recall: 0.8502874369040943 | Micro F1-Score: 0.8558676169642228\nValidation Accuracy 0.8832857142857143\n```\n# Further Direction\n- Domain specific vocabulary can be added to the models (tokenizer.add_token() for some unique words in corpus that are not present in BERT_Base vocabulary)\n\n- Different versions of BERT (BERT_Large, roBERTa, ALBERT, etc.) or other models can be experimented with. \n\n\n","metadata":{}},{"cell_type":"code","source":"\ndataset_path = ['data/Multi-Label Text Classification Dataset.csv', '/kaggle/input/multi-label-text-cls/Multi-Label Text Classification Dataset.csv'][KAGGLE]\ninterrupt_save_folder = ['interrupt', '/kaggle/working'][KAGGLE]\nsave_folder = ['saved', '/kaggle/working'][KAGGLE]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:11:23.367526Z","iopub.execute_input":"2024-04-15T17:11:23.368398Z","iopub.status.idle":"2024-04-15T17:11:23.374269Z","shell.execute_reply.started":"2024-04-15T17:11:23.368363Z","shell.execute_reply":"2024-04-15T17:11:23.373204Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_df = pd.read_csv(dataset_path)\nlabels = \"A,B,C,D,E,F,G,H,I,J,L,M,N,Z\".split(',')\nnum_labels = len(labels)\nprint(labels, num_labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:11:27.210595Z","iopub.execute_input":"2024-04-15T17:11:27.211340Z","iopub.status.idle":"2024-04-15T17:11:32.141652Z","shell.execute_reply.started":"2024-04-15T17:11:27.211307Z","shell.execute_reply":"2024-04-15T17:11:32.140746Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z'] 14\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Testing","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:11:35.118399Z","iopub.execute_input":"2024-04-15T17:11:35.118769Z","iopub.status.idle":"2024-04-15T17:11:38.309440Z","shell.execute_reply.started":"2024-04-15T17:11:35.118738Z","shell.execute_reply":"2024-04-15T17:11:38.308535Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# tokens = tokenizer.encode(\"Hello, my [MASK] is John.\")\n# mask_pos = tokens.index(tokenizer.mask_token_id)\n# print(mask_pos)\n# out = bert_model(torch.tensor([tokens]).to(device))\n# print(out.last_hidden_state.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:11:39.454714Z","iopub.execute_input":"2024-04-15T17:11:39.455249Z","iopub.status.idle":"2024-04-15T17:11:39.459467Z","shell.execute_reply.started":"2024-04-15T17:11:39.455206Z","shell.execute_reply":"2024-04-15T17:11:39.458541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Class Definitions","metadata":{}},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, sequences, labels:torch.Tensor, tokenizer):\n        self.sequences = sequences\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        char = self.tokenizer(self.sequences[idx], add_special_tokens = True,return_tensors='pt', padding='max_length', truncation=True)\n        encoded_seq = char['input_ids']\n        attention_mask = char['attention_mask']\n        #reshape the pytorch tensor to be flattened because single element\n        return encoded_seq[0], attention_mask[0], self.labels[idx]\n        # return self.encoded_seqs[idx],self.attention_masks[idx], self.labels[idx]\n\nclass BERT_Base_Multilabel(torch.nn.Module):\n    def __init__(self, num_labels): \n        \"\"\"num_labels: number of labels to classify\n           database: tuple of (X, Y) where X is a list of sentences and Y is a tensor of labels\n        \"\"\"\n        super().__init__()\n        print(\"Initializing BERT_Base_Multilabel...\")\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.cls_head = torch.nn.Sequential(\n            torch.nn.Linear(768, 1024),\n            torch.nn.ReLU(),\n            torch.nn.Linear(1024, num_labels),\n            torch.nn.Sigmoid()\n        )\n        self.loss_fn = torch.nn.BCELoss()\n        print(\"Initialized.\")\n    \n    def forward(self, encoded_seqs, attention_masks):\n        \"\"\"Input: sequence (str) of shape (batch_size, seq_len)\"\"\"\n        bert_out = self.bert(encoded_seqs, attention_mask=attention_masks)\n        clshead_output = self.cls_head(bert_out.last_hidden_state[:, 0, :]) #use the first token to classify\n        return clshead_output\n    \n    def predict(self, sequence):\n        with torch.no_grad():\n            self.eval()\n            return self.forward(sequence)\n    \n    def save(self, path):\n        torch.save(self.state_dict(), path) #save the model state dict\n\n    def load(self, path):\n        self.load_state_dict(torch.load(path))\n\n    def fit(self, epochs, batch_size, lr, dataset:torch.utils.data.Dataset, epochs_done = 0):\n        self.train()\n        \n        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        completed = epochs_done\n        try:\n            for epoch in range(epochs_done, epochs):\n                print(f\"Epoch {epoch}\")\n                pbar = tqdm(dataloader)\n                for batch in pbar:\n                    # print(\"here1\")\n                    optimizer.zero_grad()\n                    encoded_seqs, attention_masks, labels = batch\n                    encoded_seqs = encoded_seqs.to(device)\n                    attention_masks = attention_masks.to(device)\n                    labels = labels.to(device)\n                    # print(\"here2\")\n                    output = self.forward(encoded_seqs, attention_masks)\n                    loss = self.loss_fn(output, labels)\n                    # print(\"here3\")\n                    loss.backward()\n                    optimizer.step()\n                    # print(\"here4\")\n                    pbar.set_description(f\"Loss: {loss.item()}\")\n                print(f\"Epoch {epoch+1} completed. Training Loss: {loss.item()}\")\n                completed += 1\n            self.save(f\"{save_folder}/bertbaseuncased_{completed}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n            \n        except KeyboardInterrupt:\n            print(\"Training interrupted.\")\n            #save the model by date and time of interruption\n            self.save(f\"{interrupt_save_folder}/bertbaseuncased_interrupt_{epoch}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n\nclass BERT_BaseCased_Multilabel(torch.nn.Module):\n    def __init__(self, num_labels): \n        \"\"\"num_labels: number of labels to classify\n           database: tuple of (X, Y) where X is a list of sentences and Y is a tensor of labels\n        \"\"\"\n        super().__init__()\n        print(\"Initializing BERT_BaseCased_Multilabel...\")\n\n        self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n        self.cls_head = torch.nn.Sequential(\n            torch.nn.Linear(768, 1024),\n            torch.nn.ReLU(),\n            torch.nn.Linear(1024, num_labels),\n            torch.nn.Sigmoid()\n        )\n        self.loss_fn = torch.nn.BCELoss()\n        print(\"Initialized.\")\n    \n    def forward(self, encoded_seqs, attention_masks):\n        \"\"\"Input: sequence (str) of shape (batch_size, seq_len)\"\"\"\n        bert_out = self.bert(encoded_seqs, attention_mask=attention_masks)\n        clshead_output = self.cls_head(bert_out.last_hidden_state[:, 0, :]) #use the first token to classify\n        return clshead_output\n    \n    def predict(self, sequence):\n        with torch.no_grad():\n            self.eval()\n            return self.forward(sequence)\n    \n    def save(self, path):\n        torch.save(self.state_dict(), path) #save the model state dict\n\n    def load(self, path):\n        self.load_state_dict(torch.load(path))\n\n    def fit(self, epochs, batch_size, lr, dataset:torch.utils.data.Dataset, epochs_done = 0):\n        self.train()\n\n        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        completed = epochs_done\n        try:\n            for epoch in range(epochs_done, epochs):\n                print(f\"Epoch {epoch}\")\n                pbar = tqdm(dataloader)\n                for batch in pbar:\n                    # print(\"here1\")\n                    optimizer.zero_grad()\n                    encoded_seqs, attention_masks, labels = batch\n                    encoded_seqs = encoded_seqs.to(device)\n                    attention_masks = attention_masks.to(device)\n                    labels = labels.to(device)\n                    # print(\"here2\")\n                    output = self.forward(encoded_seqs, attention_masks)\n                    loss = self.loss_fn(output, labels)\n                    # print(\"here3\")\n                    loss.backward()\n                    optimizer.step()\n                    # print(\"here4\")\n                    pbar.set_description(f\"Loss: {loss.item()}\")\n                print(f\"Epoch {epoch+1} completed. Training Loss: {loss.item()}\")\n                completed += 1\n            self.save(f\"{save_folder}/bertbasecased_{completed}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n            \n        except KeyboardInterrupt:\n            print(\"Training interrupted.\")\n            #save the model by date and time of interruption\n            self.save(f\"{interrupt_save_folder}/bertbasecased_interrupt_{epoch}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n\nclass BERT_Base_Multilabel2(torch.nn.Module):\n    def __init__(self, num_labels): \n        \"\"\"num_labels: number of labels to classify\n           database: tuple of (X, Y) where X is a list of sentences and Y is a tensor of labels\n        \"\"\"\n        super().__init__()\n        print(\"Initializing BERT_Base_Multilabel...\")\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.cls_head = torch.nn.Sequential(\n            torch.nn.Dropout(0.3),\n            torch.nn.Linear(768, num_labels),\n            torch.nn.Sigmoid()\n        )\n        self.loss_fn = torch.nn.BCELoss()\n        print(\"Initialized.\")\n    \n    def forward(self, encoded_seqs, attention_masks):\n        \"\"\"Input: sequence (str) of shape (batch_size, seq_len)\"\"\"\n        bert_out = self.bert(encoded_seqs, attention_mask=attention_masks)\n        clshead_output = self.cls_head(bert_out.last_hidden_state[:, 0, :]) #use the first token to classify\n        return clshead_output\n    \n    def predict(self, sequence):\n        with torch.no_grad():\n            self.eval()\n            return self.forward(sequence)\n    \n    def save(self, path):\n        torch.save(self.state_dict(), path) #save the model state dict\n\n    def load(self, path):\n        self.load_state_dict(torch.load(path))\n\n    def fit(self, epochs, batch_size, lr, dataset:torch.utils.data.Dataset, epochs_done = 0):\n        self.train()\n        \n        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        completed = epochs_done\n        try:\n            for epoch in range(epochs_done, epochs):\n                print(f\"Epoch {epoch}\")\n                pbar = tqdm(dataloader)\n                for batch in pbar:\n                    # print(\"here1\")\n                    optimizer.zero_grad()\n                    encoded_seqs, attention_masks, labels = batch\n                    encoded_seqs = encoded_seqs.to(device)\n                    attention_masks = attention_masks.to(device)\n                    labels = labels.to(device)\n                    # print(\"here2\")\n                    output = self.forward(encoded_seqs, attention_masks)\n                    loss = self.loss_fn(output, labels)\n                    # print(\"here3\")\n                    loss.backward()\n                    optimizer.step()\n                    # print(\"here4\")\n                    pbar.set_description(f\"Loss: {loss.item()}\")\n                print(f\"Epoch {epoch+1} completed. Training Loss: {loss.item()}\")\n                completed += 1\n            self.save(f\"{save_folder}/bertbaseuncased2_{completed}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n            \n        except KeyboardInterrupt:\n            print(\"Training interrupted.\")\n            #save the model by date and time of interruption\n            self.save(f\"{interrupt_save_folder}/bertbaseuncased2_interrupt_{epoch}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:44:02.596756Z","iopub.execute_input":"2024-04-15T17:44:02.597084Z","iopub.status.idle":"2024-04-15T17:44:02.635388Z","shell.execute_reply.started":"2024-04-15T17:44:02.597059Z","shell.execute_reply":"2024-04-15T17:44:02.634487Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Prepare database\nconcat_text = []\nfor i in range(data_df.shape[0]):\n    concat_text.append(f\"Title : {data_df.iloc[i].Title}; Abstract : {data_df.iloc[i].abstractText}\")\ndata_df['text'] = pd.Series(concat_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:12:43.311115Z","iopub.execute_input":"2024-04-15T17:12:43.311701Z","iopub.status.idle":"2024-04-15T17:12:50.992455Z","shell.execute_reply.started":"2024-04-15T17:12:43.311671Z","shell.execute_reply":"2024-04-15T17:12:50.991517Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(\"Number of sequences:\", len(data_df))\ntexts = list(data_df['text'].values)\ntarget = torch.tensor(data_df[labels].values).float()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:13:02.825180Z","iopub.execute_input":"2024-04-15T17:13:02.825560Z","iopub.status.idle":"2024-04-15T17:13:02.876123Z","shell.execute_reply.started":"2024-04-15T17:13:02.825531Z","shell.execute_reply":"2024-04-15T17:13:02.875196Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Number of sequences: 50000\n","output_type":"stream"}]},{"cell_type":"code","source":"# cls_model = BERT_Base_Multilabel(num_labels, (texts, data_df[labels].values))\n# short = texts[1]\n# encoded = tokenizer(short,add_special_tokens=True ,return_tensors='pt', padding='max_length', truncation=True)\n# out = bert_model(encoded['input_ids'].to(device), encoded['attention_mask'].to(device))\n# print(out[1].shape)\n\n# short_dataset = Dataset(texts[:10], target[:10], tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:13:05.992884Z","iopub.execute_input":"2024-04-15T17:13:05.993852Z","iopub.status.idle":"2024-04-15T17:13:05.998394Z","shell.execute_reply.started":"2024-04-15T17:13:05.993810Z","shell.execute_reply":"2024-04-15T17:13:05.997205Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Trying Variations","metadata":{}},{"cell_type":"markdown","source":"## Dataset Prep","metadata":{}},{"cell_type":"code","source":"def construct_dataset(texts, target, tokenizer):\n    \"\"\"\n    Input: texts: list of strings, target: tensor of shape (num_samples, num_labels)\n    Output: full_dataset, train_dataset, val_dataset, test_dataset\n    \"\"\"\n    full_dataset = Dataset(texts, target, tokenizer)\n    train_size = int(0.8 * len(full_dataset))\n    val_size = int(0.1 * len(full_dataset))\n    test_size = len(full_dataset) - train_size - val_size\n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n    train_dataset.tokenizer = tokenizer\n    val_dataset.tokenizer = tokenizer\n    test_dataset.tokenizer = tokenizer\n    return full_dataset, train_dataset, val_dataset, test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:13:13.722954Z","iopub.execute_input":"2024-04-15T17:13:13.723442Z","iopub.status.idle":"2024-04-15T17:13:13.730288Z","shell.execute_reply.started":"2024-04-15T17:13:13.723411Z","shell.execute_reply":"2024-04-15T17:13:13.729284Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataset):\n    # Evaluate Accuracy\n    data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n    model.eval()\n    metrics = {}\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n            encoded_seqs, attention_masks, targets = batch\n            encoded_seqs = encoded_seqs.to(device)\n            attention_masks = attention_masks.to(device)\n            targets = targets.to(device)\n\n            outputs = model(encoded_seqs, attention_masks)\n            \n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n    accuracy = 0\n    for i in range(len(fin_outputs)):\n        accuracy += np.sum(np.round(fin_outputs[i]) == fin_targets[i])\n    accuracy = accuracy/(len(fin_outputs)*len(fin_outputs[0]))\n    # Evaluate Precision, Recall and F1-Score for each class separately\n    fin_outputs = np.round(fin_outputs)\n    fin_targets = np.array(fin_targets)\n    fin_outputs = np.array(fin_outputs)\n    fin_targets = np.array(fin_targets)\n    precisions = {}\n    recalls = {}\n    f1_scores = {}\n    for label in range(len(labels)):\n        precision = np.sum((fin_outputs[:, label] == 1) & (fin_targets[:, label] == 1)) / np.sum(fin_outputs[:, label] == 1)\n        recall = np.sum((fin_outputs[:, label] == 1) & (fin_targets[:, label] == 1)) / np.sum(fin_targets[:, label] == 1)\n        f1_score = 2 * precision * recall / (precision + recall)\n        precisions[label] = precision\n        recalls[label] = recall\n        f1_scores[label] = f1_score\n        print(f\"Label: {labels[label]} | Precision: {precision} | Recall: {recall} | F1-Score: {f1_score}\")\n    # Evaluate Macro Average and Micro Average Precision, Recall and F1-Score\n    macro_precision = np.mean(list(precisions.values()))\n    macro_recall = np.mean(list(recalls.values()))\n    macro_f1_score = np.mean(list(f1_scores.values()))\n    micro_precision = np.sum([np.sum((fin_outputs[:, label] == 1) & (fin_targets[:, label] == 1)) for label in range(len(labels))]) / np.sum(fin_outputs == 1)\n    micro_recall = np.sum([np.sum((fin_outputs[:, label] == 1) & (fin_targets[:, label] == 1)) for label in range(len(labels))]) / np.sum(fin_targets == 1)\n    micro_f1_score = 2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n    print(f\"Macro Precision: {macro_precision} | Macro Recall: {macro_recall} | Macro F1-Score: {macro_f1_score}\")\n    print(f\"Micro Precision: {micro_precision} | Micro Recall: {micro_recall} | Micro F1-Score: {micro_f1_score}\")\n    metrics['accuracy'] = accuracy\n    metrics['precisions'] = precisions\n    metrics['recalls'] = recalls\n    metrics['f1_scores'] = f1_scores\n    metrics['macro_precision'] = macro_precision\n    metrics['macro_recall'] = macro_recall\n    metrics['macro_f1_score'] = macro_f1_score\n    metrics['micro_precision'] = micro_precision\n    metrics['micro_recall'] = micro_recall\n    metrics['micro_f1_score'] = micro_f1_score\n    return fin_outputs, fin_targets, metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:34:15.488168Z","iopub.execute_input":"2024-04-15T17:34:15.488576Z","iopub.status.idle":"2024-04-15T17:34:15.506175Z","shell.execute_reply.started":"2024-04-15T17:34:15.488547Z","shell.execute_reply":"2024-04-15T17:34:15.505262Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## BERT Base Uncased [BERT Freezed]","metadata":{}},{"cell_type":"code","source":"cls_model = BERT_Base_Multilabel(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, cls_model.tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:13:45.329356Z","iopub.execute_input":"2024-04-15T17:13:45.330316Z","iopub.status.idle":"2024-04-15T17:13:46.417791Z","shell.execute_reply.started":"2024-04-15T17:13:45.330280Z","shell.execute_reply":"2024-04-15T17:13:46.416496Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Initializing BERT_Base_Multilabel...\nInitialized.\n","output_type":"stream"}]},{"cell_type":"code","source":"#freeze all BERT params\nfor param in cls_model.bert.parameters():\n    param.requires_grad = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#continue training\ncls_model.fit(1, 32, 1e-4, train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate(cls_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT Base Cased - [Bert Freezed]","metadata":{}},{"cell_type":"code","source":"# Trying BERT_cased for 1 epoch\n\ncls_bertcased = BERT_BaseCased_Multilabel(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, cls_bertcased.tokenizer)\nprint(cls_bertcased)\n#freeze all BERT params\nfor params in cls_bertcased.bert.parameters():\n    params.requires_grad = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_bertcased.fit(1, 32, 1e-4, train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = evaluate(cls_bertcased, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Uncased BERTBase [Last Encoder Layer Unfrozen]","metadata":{}},{"cell_type":"code","source":"base_unfreeze1 =  BERT_BaseCased_Multilabel(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, base_unfreeze1.tokenizer)\n\ncnt = 0\nfor param in base_unfreeze1.bert.parameters():\n    param.requires_grad = False\n    \nfor layer in base_unfreeze1.bert.encoder.layer:\n    cnt += 1\n    if cnt >= 12:\n        for param in layer.parameters():\n            param.required_grad = True\n\nprint(\"Last 1 Encoder Layer Unfreezed\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_unfreeze1.fit(1, 32, 1e-4, train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy_base_unfreeze1 = evaluate(base_unfreeze1, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lower the learning rate and train for some more time\nbase_unfreeze1.load(\"/kaggle/working/bertbasecased_1_2024-04-14_15:14:06.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_unfreeze1.fit(2, 32, 2e-5, train_dataset, epochs_done=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_3 = evaluate(base_unfreeze1, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unfreezing Embedding Layer","metadata":{}},{"cell_type":"code","source":"\nuncased_uf_emb_enc = BERT_Base_Multilabel(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, uncased_uf_emb_enc.tokenizer)\n\ncnt = 0\nfor param in uncased_uf_emb_enc.bert.parameters():\n    param.requires_grad = False\n    \nfor layer in uncased_uf_emb_enc.bert.encoder.layer:\n    cnt += 1\n    if cnt >= 12:\n        for param in layer.parameters():\n            param.required_grad = True\n\nprint(\"Last 1 Encoder Layer Unfreezed\")\nfor param in uncased_uf_emb_enc.bert.embeddings.parameters():\n    param.requires_grad = True\n\nprint(\"Embedding Layer Unfreezed\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uncased_uf_emb_enc.fit(1,16,1e-4, train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(uncased_uf_emb_enc, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unfreeze one more layer and do 2 more epochs\nuncased_uf_emb_enc.load(\"/kaggle/working/bertbaseuncased_interrupt_1_2024-04-14_17:29:54.pt\")\ncnt = 0\nfor param in uncased_uf_emb_enc.bert.parameters():\n    param.requires_grad = False\n    \nfor layer in uncased_uf_emb_enc.bert.encoder.layer:\n    cnt += 1\n    if cnt >= 11:\n        for param in layer.parameters():\n            param.required_grad = True\n\nprint(\"Last 2 Encoder Layer Unfreezed\")\nfor param in uncased_uf_emb_enc.bert.embeddings.parameters():\n    param.requires_grad = True\n\nprint(\"Embedding Layer Unfreezed\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uncased_uf_emb_enc.fit(2,16,2e-5, train_dataset, epochs_done=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(uncased_uf_emb_enc, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying the above (uncased, unfreezed embeddings) without unfreezing the encoder layer\nTo confirm whether unfreezing encoder actually makes a difference or not","metadata":{}},{"cell_type":"code","source":"uncased_uf_emb = BERT_Base_Multilabel(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, uncased_uf_emb.tokenizer)\n\ncnt = 0\nfor param in uncased_uf_emb.bert.parameters():\n    param.requires_grad = False\n    \n# for layer in uncased_uf_emb.bert.encoder.layer:\n#     cnt += 1\n#     if cnt >= 12:\n#         for param in layer.parameters():\n#             param.required_grad = True\n\n# print(\"Last 1 Encoder Layer Unfreezed\")\nfor param in uncased_uf_emb.bert.embeddings.parameters():\n    param.requires_grad = True\n\nprint(\"Embedding Layer Unfreezed\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uncased_uf_emb.fit(1,16,1e-4, train_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detailed Evaluation","metadata":{}},{"cell_type":"code","source":"uncased_uf_emb = BERT_Base_Multilabel(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, uncased_uf_emb.tokenizer)\n\nuncased_uf_emb.load(\"/kaggle/input/bert_base_multilabel_classifier/pytorch/uf_emb_enc/1/e3_2024-04-14_18_37_33.pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:15:23.823290Z","iopub.execute_input":"2024-04-15T17:15:23.823978Z","iopub.status.idle":"2024-04-15T17:15:33.392039Z","shell.execute_reply.started":"2024-04-15T17:15:23.823945Z","shell.execute_reply":"2024-04-15T17:15:33.391227Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Initializing BERT_Base_Multilabel...\nInitialized.\n","output_type":"stream"}]},{"cell_type":"code","source":"_, _, test_report = evaluate(uncased_uf_emb, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:34:23.872333Z","iopub.execute_input":"2024-04-15T17:34:23.872680Z","iopub.status.idle":"2024-04-15T17:36:41.006684Z","shell.execute_reply.started":"2024-04-15T17:34:23.872653Z","shell.execute_reply":"2024-04-15T17:36:41.005771Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"100%|██████████| 1250/1250 [02:17<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Label: A | Precision: 0.8285097192224622 | Recall: 0.8382867132867133 | F1-Score: 0.8333695416033022\nLabel: B | Precision: 0.9719195305951384 | Recall: 0.9906023067065357 | F1-Score: 0.9811719906917709\nLabel: C | Precision: 0.8559498956158664 | Recall: 0.9439754412893323 | F1-Score: 0.8978102189781022\nLabel: D | Precision: 0.9496774193548387 | Recall: 0.9278285534194768 | F1-Score: 0.9386258568468037\nLabel: E | Precision: 0.8502923976608188 | Recall: 0.9318123558062036 | F1-Score: 0.889187866927593\nLabel: F | Precision: 0.8434886499402628 | Recall: 0.800453514739229 | F1-Score: 0.8214077952297848\nLabel: G | Precision: 0.8388533259114945 | Recall: 0.8880377136122569 | F1-Score: 0.8627450980392156\nLabel: H | Precision: 0.6534653465346535 | Recall: 0.10361067503924647 | F1-Score: 0.17886178861788615\nLabel: I | Precision: 0.7377049180327869 | Recall: 0.5833333333333334 | F1-Score: 0.6514994829369183\nLabel: J | Precision: 0.7937853107344632 | Recall: 0.5146520146520146 | F1-Score: 0.6244444444444444\nLabel: L | Precision: 0.8074074074074075 | Recall: 0.4497936726272352 | F1-Score: 0.5777385159010601\nLabel: M | Precision: 0.8991788321167883 | Recall: 0.9214586255259467 | F1-Score: 0.9101824059108752\nLabel: N | Precision: 0.8334075723830735 | Recall: 0.80995670995671 | F1-Score: 0.8215148188803513\nLabel: Z | Precision: 0.8573551263001485 | Recall: 0.7230576441102757 | F1-Score: 0.7845003399048267\nMacro Precision: 0.837213960843586 | Macro Recall: 0.7447756624360363 | Macro F1-Score: 0.7695042974937811\nMicro Precision: 0.8778539218123069 | Micro Recall: 0.8637459385808616 | Micro F1-Score: 0.8707427887155285\n","output_type":"stream"}]},{"cell_type":"code","source":"print(test_report['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:38:48.539864Z","iopub.execute_input":"2024-04-15T17:38:48.540199Z","iopub.status.idle":"2024-04-15T17:38:48.545283Z","shell.execute_reply.started":"2024-04-15T17:38:48.540174Z","shell.execute_reply":"2024-04-15T17:38:48.544367Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"0.8951428571428571\n","output_type":"stream"}]},{"cell_type":"code","source":"_, _, val_report = evaluate(uncased_uf_emb, val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:39:18.364436Z","iopub.execute_input":"2024-04-15T17:39:18.365139Z","iopub.status.idle":"2024-04-15T17:41:34.874954Z","shell.execute_reply.started":"2024-04-15T17:39:18.365108Z","shell.execute_reply":"2024-04-15T17:41:34.874023Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 1250/1250 [02:16<00:00,  9.17it/s]","output_type":"stream"},{"name":"stdout","text":"Label: A | Precision: 0.8407833120476799 | Recall: 0.836155800169348 | F1-Score: 0.8384631713012098\nLabel: B | Precision: 0.9690526315789474 | Recall: 0.9911714039621017 | F1-Score: 0.9799872258888653\nLabel: C | Precision: 0.8568953568953569 | Recall: 0.9435330026707364 | F1-Score: 0.8981296531686943\nLabel: D | Precision: 0.9467146126185028 | Recall: 0.9243536546441111 | F1-Score: 0.9354005167958656\nLabel: E | Precision: 0.8542356838618078 | Recall: 0.9228016359918201 | F1-Score: 0.8871958712214303\nLabel: F | Precision: 0.8127250900360145 | Recall: 0.8166465621230398 | F1-Score: 0.8146811070998796\nLabel: G | Precision: 0.8505970563732297 | Recall: 0.9043401240035429 | F1-Score: 0.8766456783056669\nLabel: H | Precision: 0.6428571428571429 | Recall: 0.11707317073170732 | F1-Score: 0.19807427785419532\nLabel: I | Precision: 0.753393665158371 | Recall: 0.6294896030245747 | F1-Score: 0.685890834191555\nLabel: J | Precision: 0.7595628415300546 | Recall: 0.556 | F1-Score: 0.6420323325635104\nLabel: L | Precision: 0.8072289156626506 | Recall: 0.44966442953020136 | F1-Score: 0.5775862068965518\nLabel: M | Precision: 0.8943694741740345 | Recall: 0.9218225419664269 | F1-Score: 0.9078885214926783\nLabel: N | Precision: 0.834759117514633 | Recall: 0.8362652232746955 | F1-Score: 0.8355114916629112\nLabel: Z | Precision: 0.8485294117647059 | Recall: 0.7513020833333334 | F1-Score: 0.7969613259668509\nMacro Precision: 0.8336931651480807 | Macro Recall: 0.7571870882446886 | Macro F1-Score: 0.7767463010292762\nMicro Precision: 0.8782085513902239 | Micro Recall: 0.8702155430909796 | Micro F1-Score: 0.8741937770217592\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(val_report['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:42:59.223567Z","iopub.execute_input":"2024-04-15T17:42:59.224339Z","iopub.status.idle":"2024-04-15T17:42:59.229848Z","shell.execute_reply.started":"2024-04-15T17:42:59.224299Z","shell.execute_reply":"2024-04-15T17:42:59.228511Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"0.8985714285714286\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dropout Layers","metadata":{}},{"cell_type":"code","source":"uncased2_uf_emb = BERT_Base_Multilabel2(num_labels).to(device)\nfull_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, uncased2_uf_emb.tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:44:11.794164Z","iopub.execute_input":"2024-04-15T17:44:11.794524Z","iopub.status.idle":"2024-04-15T17:44:12.580923Z","shell.execute_reply.started":"2024-04-15T17:44:11.794497Z","shell.execute_reply":"2024-04-15T17:44:12.579815Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Initializing BERT_Base_Multilabel...\nInitialized.\n","output_type":"stream"}]},{"cell_type":"code","source":"uncased2_uf_emb.fit(1,16,1e-4, train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:44:17.062889Z","iopub.execute_input":"2024-04-15T17:44:17.063274Z","iopub.status.idle":"2024-04-15T18:25:24.351793Z","shell.execute_reply.started":"2024-04-15T17:44:17.063242Z","shell.execute_reply":"2024-04-15T18:25:24.350747Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 0\n","output_type":"stream"},{"name":"stderr","text":"Loss: 0.29174670577049255: 100%|██████████| 2500/2500 [41:06<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed. Training Loss: 0.29174670577049255\n","output_type":"stream"}]},{"cell_type":"code","source":"_, _, test_report = evaluate(uncased2_uf_emb, test_dataset)\nprint(\"Test Accuracy\", test_report['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:26:11.184272Z","iopub.execute_input":"2024-04-15T18:26:11.184930Z","iopub.status.idle":"2024-04-15T18:28:28.825582Z","shell.execute_reply.started":"2024-04-15T18:26:11.184900Z","shell.execute_reply":"2024-04-15T18:28:28.824593Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 1250/1250 [02:17<00:00,  9.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Label: A | Precision: 0.8210930828351836 | Recall: 0.8207426376440461 | F1-Score: 0.8209178228388474\nLabel: B | Precision: 0.9713916971391697 | Recall: 0.9764324324324324 | F1-Score: 0.9739055423765366\nLabel: C | Precision: 0.8890631125049 | Recall: 0.8692985818321196 | F1-Score: 0.8790697674418604\nLabel: D | Precision: 0.9225642653125992 | Recall: 0.9314322332585709 | F1-Score: 0.9269770408163265\nLabel: E | Precision: 0.8126901347240331 | Recall: 0.9572562068082928 | F1-Score: 0.8790692208250087\nLabel: F | Precision: 0.8943089430894309 | Recall: 0.6307339449541285 | F1-Score: 0.7397444519166106\nLabel: G | Precision: 0.8169051404345522 | Recall: 0.9099763872491146 | F1-Score: 0.8609327003630272\nLabel: H | Precision: 0.5253164556962026 | Recall: 0.13651315789473684 | F1-Score: 0.21671018276762402\nLabel: I | Precision: 0.7130620985010707 | Recall: 0.6098901098901099 | F1-Score: 0.6574531095755183\nLabel: J | Precision: 0.7045454545454546 | Recall: 0.5646630236794171 | F1-Score: 0.6268958543983822\nLabel: L | Precision: 0.6530303030303031 | Recall: 0.5693527080581242 | F1-Score: 0.6083274523641496\nLabel: M | Precision: 0.8991470145509283 | Recall: 0.847682119205298 | F1-Score: 0.8726564402240078\nLabel: N | Precision: 0.8277010947168015 | Recall: 0.7763392857142857 | F1-Score: 0.8011978806726561\nLabel: Z | Precision: 0.8527272727272728 | Recall: 0.6051612903225806 | F1-Score: 0.7079245283018867\nMacro Precision: 0.8073961478434216 | Macro Recall: 0.7289624370673756 | Macro F1-Score: 0.7551272853487457\nMicro Precision: 0.8606566142658539 | Micro Recall: 0.8485274478105012 | Micro F1-Score: 0.8545489939299555\nTest Accuracy 0.8825857142857143\n","output_type":"stream"}]},{"cell_type":"code","source":"_, _, val_report = evaluate(uncased2_uf_emb, val_dataset)\nprint(\"Validation Accuracy\", val_report['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:28:41.729045Z","iopub.execute_input":"2024-04-15T18:28:41.729918Z","iopub.status.idle":"2024-04-15T18:31:00.748335Z","shell.execute_reply.started":"2024-04-15T18:28:41.729881Z","shell.execute_reply":"2024-04-15T18:31:00.747330Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"100%|██████████| 1250/1250 [02:18<00:00,  9.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Label: A | Precision: 0.8199152542372882 | Recall: 0.8255119453924915 | F1-Score: 0.8227040816326532\nLabel: B | Precision: 0.9751712328767124 | Recall: 0.9735042735042735 | F1-Score: 0.9743370402053038\nLabel: C | Precision: 0.9089093088294047 | Recall: 0.8627227910504361 | F1-Score: 0.8852140077821012\nLabel: D | Precision: 0.9135802469135802 | Recall: 0.925 | F1-Score: 0.9192546583850932\nLabel: E | Precision: 0.817765168048887 | Recall: 0.9563552833078101 | F1-Score: 0.8816470588235295\nLabel: F | Precision: 0.8991869918699187 | Recall: 0.6415313225058005 | F1-Score: 0.7488151658767772\nLabel: G | Precision: 0.8150470219435737 | Recall: 0.9217134416543574 | F1-Score: 0.8651046721197837\nLabel: H | Precision: 0.5297619047619048 | Recall: 0.15724381625441697 | F1-Score: 0.24250681198910082\nLabel: I | Precision: 0.7215777262180975 | Recall: 0.5791433891992551 | F1-Score: 0.6425619834710744\nLabel: J | Precision: 0.703962703962704 | Recall: 0.5666041275797373 | F1-Score: 0.6278586278586279\nLabel: L | Precision: 0.65625 | Recall: 0.5976714100905562 | F1-Score: 0.6255924170616113\nLabel: M | Precision: 0.8966550174737893 | Recall: 0.8499763369616659 | F1-Score: 0.8726919339164237\nLabel: N | Precision: 0.8230297310051912 | Recall: 0.7625710537822474 | F1-Score: 0.7916477530640037\nLabel: Z | Precision: 0.8250428816466552 | Recall: 0.6222509702457956 | F1-Score: 0.7094395280235988\nMacro Precision: 0.8075610849848361 | Macro Recall: 0.7315571543949175 | Macro F1-Score: 0.7578125528721201\nMicro Precision: 0.8615215229436 | Micro Recall: 0.8502874369040943 | Micro F1-Score: 0.8558676169642228\nValidation Accuracy 0.8832857142857143\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}