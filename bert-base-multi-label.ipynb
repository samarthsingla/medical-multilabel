{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:02:53.753938Z","iopub.status.busy":"2024-04-14T14:02:53.753283Z","iopub.status.idle":"2024-04-14T14:02:53.762465Z","shell.execute_reply":"2024-04-14T14:02:53.761525Z","shell.execute_reply.started":"2024-04-14T14:02:53.753902Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MPS not available because the current PyTorch install was not built with MPS enabled.\n","Current Device cuda\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","#set device to cuda, if not available check mps else use cpu\n","if not torch.backends.mps.is_available():\n","    if not torch.backends.mps.is_built():\n","        print(\"MPS not available because the current PyTorch install was not \"\n","              \"built with MPS enabled.\")\n","    else:\n","        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n","              \"and/or you do not have an MPS-enabled device on this machine.\")\n","    if torch.cuda.is_available():\n","        device = torch.device('cuda')\n","    else:\n","        device = torch.device('cpu')\n","else:\n","    device = torch.device('mps')\n","\n","    \n","#print device type\n","print(\"Current Device\", device)\n","torch.manual_seed(0)\n","KAGGLE = 1\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:02:56.331032Z","iopub.status.busy":"2024-04-14T14:02:56.330337Z","iopub.status.idle":"2024-04-14T14:02:56.335629Z","shell.execute_reply":"2024-04-14T14:02:56.334591Z","shell.execute_reply.started":"2024-04-14T14:02:56.330994Z"},"trusted":true},"outputs":[],"source":["\n","dataset_path = ['data/Multi-Label Text Classification Dataset.csv', '/kaggle/input/multi-label-text-cls/Multi-Label Text Classification Dataset.csv'][KAGGLE]\n","interrupt_save_folder = ['interrupt', '/kaggle/working'][KAGGLE]\n","save_folder = ['saved', '/kaggle/working'][KAGGLE]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:02:57.982251Z","iopub.status.busy":"2024-04-14T14:02:57.981864Z","iopub.status.idle":"2024-04-14T14:03:01.040406Z","shell.execute_reply":"2024-04-14T14:03:01.039389Z","shell.execute_reply.started":"2024-04-14T14:02:57.982204Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z'] 14\n"]}],"source":["data_df = pd.read_csv(dataset_path)\n","labels = \"A,B,C,D,E,F,G,H,I,J,L,M,N,Z\".split(',')\n","num_labels = len(labels)\n","print(labels, num_labels)"]},{"cell_type":"markdown","metadata":{},"source":["### Testing"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:03:02.283418Z","iopub.status.busy":"2024-04-14T14:03:02.283060Z","iopub.status.idle":"2024-04-14T14:03:05.173362Z","shell.execute_reply":"2024-04-14T14:03:05.172540Z","shell.execute_reply.started":"2024-04-14T14:03:02.283389Z"},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertModel\n","\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","# bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:03:08.735819Z","iopub.status.busy":"2024-04-14T14:03:08.735252Z","iopub.status.idle":"2024-04-14T14:03:08.740062Z","shell.execute_reply":"2024-04-14T14:03:08.739039Z","shell.execute_reply.started":"2024-04-14T14:03:08.735787Z"},"trusted":true},"outputs":[],"source":["# tokens = tokenizer.encode(\"Hello, my [MASK] is John.\")\n","# mask_pos = tokens.index(tokenizer.mask_token_id)\n","# print(mask_pos)\n","# out = bert_model(torch.tensor([tokens]).to(device))\n","# print(out.last_hidden_state.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Class Definitions"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:10:30.552165Z","iopub.status.busy":"2024-04-14T14:10:30.551746Z","iopub.status.idle":"2024-04-14T14:10:30.581032Z","shell.execute_reply":"2024-04-14T14:10:30.580074Z","shell.execute_reply.started":"2024-04-14T14:10:30.552135Z"},"trusted":true},"outputs":[],"source":["import time\n","from tqdm import tqdm\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, sequences, labels:torch.Tensor, tokenizer):\n","        self.sequences = sequences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","    \n","    def __getitem__(self, idx):\n","        char = self.tokenizer(self.sequences[idx], add_special_tokens = True,return_tensors='pt', padding='max_length', truncation=True)\n","        encoded_seq = char['input_ids']\n","        attention_mask = char['attention_mask']\n","        #reshape the pytorch tensor to be flattened because single element\n","        return encoded_seq[0], attention_mask[0], self.labels[idx]\n","        # return self.encoded_seqs[idx],self.attention_masks[idx], self.labels[idx]\n","\n","class BERT_Base_Multilabel(torch.nn.Module):\n","    def __init__(self, num_labels): \n","        \"\"\"num_labels: number of labels to classify\n","           database: tuple of (X, Y) where X is a list of sentences and Y is a tensor of labels\n","        \"\"\"\n","        super().__init__()\n","        print(\"Initializing BERT_Base_Multilabel...\")\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.cls_head = torch.nn.Sequential(\n","            torch.nn.Linear(768, 1024),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(1024, num_labels),\n","            torch.nn.Sigmoid()\n","        )\n","        self.loss_fn = torch.nn.BCELoss()\n","        print(\"Initialized.\")\n","    \n","    def forward(self, encoded_seqs, attention_masks):\n","        \"\"\"Input: sequence (str) of shape (batch_size, seq_len)\"\"\"\n","        bert_out = self.bert(encoded_seqs, attention_mask=attention_masks)\n","        clshead_output = self.cls_head(bert_out.last_hidden_state[:, 0, :]) #use the first token to classify\n","        return clshead_output\n","    \n","    def predict(self, sequence):\n","        with torch.no_grad():\n","            self.eval()\n","            return self.forward(sequence)\n","    \n","    def save(self, path):\n","        torch.save(self.state_dict(), path) #save the model state dict\n","\n","    def load(self, path):\n","        self.load_state_dict(torch.load(path))\n","\n","    def fit(self, epochs, batch_size, lr, dataset:torch.utils.data.Dataset, epochs_done = 0):\n","        self.train()\n","        \n","        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n","        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","        completed = epochs_done\n","        try:\n","            for epoch in range(epochs_done, epochs):\n","                print(f\"Epoch {epoch}\")\n","                pbar = tqdm(dataloader)\n","                for batch in pbar:\n","                    # print(\"here1\")\n","                    optimizer.zero_grad()\n","                    encoded_seqs, attention_masks, labels = batch\n","                    encoded_seqs = encoded_seqs.to(device)\n","                    attention_masks = attention_masks.to(device)\n","                    labels = labels.to(device)\n","                    # print(\"here2\")\n","                    output = self.forward(encoded_seqs, attention_masks)\n","                    loss = self.loss_fn(output, labels)\n","                    # print(\"here3\")\n","                    loss.backward()\n","                    optimizer.step()\n","                    # print(\"here4\")\n","                    pbar.set_description(f\"Loss: {loss.item()}\")\n","                print(f\"Epoch {epoch+1} completed. Training Loss: {loss.item()}\")\n","                completed += 1\n","            self.save(f\"{save_folder}/bertbaseuncased_{completed}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n","            \n","        except KeyboardInterrupt:\n","            print(\"Training interrupted.\")\n","            #save the model by date and time of interruption\n","            self.save(f\"{interrupt_save_folder}/bertbaseuncased_interrupt_{epoch}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n","\n","class BERT_BaseCased_Multilabel(torch.nn.Module):\n","    def __init__(self, num_labels): \n","        \"\"\"num_labels: number of labels to classify\n","           database: tuple of (X, Y) where X is a list of sentences and Y is a tensor of labels\n","        \"\"\"\n","        super().__init__()\n","        print(\"Initializing BERT_BaseCased_Multilabel...\")\n","\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","        self.cls_head = torch.nn.Sequential(\n","            torch.nn.Linear(768, 1024),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(1024, num_labels),\n","            torch.nn.Sigmoid()\n","        )\n","        self.loss_fn = torch.nn.BCELoss()\n","        print(\"Initialized.\")\n","    \n","    def forward(self, encoded_seqs, attention_masks):\n","        \"\"\"Input: sequence (str) of shape (batch_size, seq_len)\"\"\"\n","        bert_out = self.bert(encoded_seqs, attention_mask=attention_masks)\n","        clshead_output = self.cls_head(bert_out.last_hidden_state[:, 0, :]) #use the first token to classify\n","        return clshead_output\n","    \n","    def predict(self, sequence):\n","        with torch.no_grad():\n","            self.eval()\n","            return self.forward(sequence)\n","    \n","    def save(self, path):\n","        torch.save(self.state_dict(), path) #save the model state dict\n","\n","    def load(self, path):\n","        self.load_state_dict(torch.load(path))\n","\n","    def fit(self, epochs, batch_size, lr, dataset:torch.utils.data.Dataset, epochs_done = 0):\n","        self.train()\n","\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n","        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","        completed = epochs_done\n","        try:\n","            for epoch in range(epochs_done, epochs):\n","                print(f\"Epoch {epoch}\")\n","                pbar = tqdm(dataloader)\n","                for batch in pbar:\n","                    # print(\"here1\")\n","                    optimizer.zero_grad()\n","                    encoded_seqs, attention_masks, labels = batch\n","                    encoded_seqs = encoded_seqs.to(device)\n","                    attention_masks = attention_masks.to(device)\n","                    labels = labels.to(device)\n","                    # print(\"here2\")\n","                    output = self.forward(encoded_seqs, attention_masks)\n","                    loss = self.loss_fn(output, labels)\n","                    # print(\"here3\")\n","                    loss.backward()\n","                    optimizer.step()\n","                    # print(\"here4\")\n","                    pbar.set_description(f\"Loss: {loss.item()}\")\n","                print(f\"Epoch {epoch+1} completed. Training Loss: {loss.item()}\")\n","                completed += 1\n","            self.save(f\"{save_folder}/bertbasecased_{completed}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n","            \n","        except KeyboardInterrupt:\n","            print(\"Training interrupted.\")\n","            #save the model by date and time of interruption\n","            self.save(f\"{interrupt_save_folder}/bertbasecased_interrupt_{epoch}_{time.strftime('%Y-%m-%d_%H:%M:%S')}.pt\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:03:20.357088Z","iopub.status.busy":"2024-04-14T14:03:20.356406Z","iopub.status.idle":"2024-04-14T14:03:27.996428Z","shell.execute_reply":"2024-04-14T14:03:27.995657Z","shell.execute_reply.started":"2024-04-14T14:03:20.357059Z"},"trusted":true},"outputs":[],"source":["#Prepare database\n","concat_text = []\n","for i in range(data_df.shape[0]):\n","    concat_text.append(f\"Title : {data_df.iloc[i].Title}; Abstract : {data_df.iloc[i].abstractText}\")\n","data_df['text'] = pd.Series(concat_text)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:03:27.998062Z","iopub.status.busy":"2024-04-14T14:03:27.997799Z","iopub.status.idle":"2024-04-14T14:03:28.042870Z","shell.execute_reply":"2024-04-14T14:03:28.041968Z","shell.execute_reply.started":"2024-04-14T14:03:27.998039Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of sequences: 50000\n"]}],"source":["print(\"Number of sequences:\", len(data_df))\n","texts = list(data_df['text'].values)\n","target = torch.tensor(data_df[labels].values).float()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:03:31.160175Z","iopub.status.busy":"2024-04-14T14:03:31.159499Z","iopub.status.idle":"2024-04-14T14:03:31.164344Z","shell.execute_reply":"2024-04-14T14:03:31.163364Z","shell.execute_reply.started":"2024-04-14T14:03:31.160143Z"},"trusted":true},"outputs":[],"source":["# cls_model = BERT_Base_Multilabel(num_labels, (texts, data_df[labels].values))\n","# short = texts[1]\n","# encoded = tokenizer(short,add_special_tokens=True ,return_tensors='pt', padding='max_length', truncation=True)\n","# out = bert_model(encoded['input_ids'].to(device), encoded['attention_mask'].to(device))\n","# print(out[1].shape)\n","\n","# short_dataset = Dataset(texts[:10], target[:10], tokenizer)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Trying Variations"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Prep"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:10:42.511832Z","iopub.status.busy":"2024-04-14T14:10:42.511089Z","iopub.status.idle":"2024-04-14T14:10:42.517965Z","shell.execute_reply":"2024-04-14T14:10:42.516995Z","shell.execute_reply.started":"2024-04-14T14:10:42.511798Z"},"trusted":true},"outputs":[],"source":["def construct_dataset(texts, target, tokenizer):\n","    \"\"\"\n","    Input: texts: list of strings, target: tensor of shape (num_samples, num_labels)\n","    Output: full_dataset, train_dataset, val_dataset, test_dataset\n","    \"\"\"\n","    full_dataset = Dataset(texts, target, tokenizer)\n","    train_size = int(0.8 * len(full_dataset))\n","    val_size = int(0.1 * len(full_dataset))\n","    test_size = len(full_dataset) - train_size - val_size\n","    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n","    train_dataset.tokenizer = tokenizer\n","    val_dataset.tokenizer = tokenizer\n","    test_dataset.tokenizer = tokenizer\n","    return full_dataset, train_dataset, val_dataset, test_dataset"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:10:43.256953Z","iopub.status.busy":"2024-04-14T14:10:43.256193Z","iopub.status.idle":"2024-04-14T14:10:43.264497Z","shell.execute_reply":"2024-04-14T14:10:43.263517Z","shell.execute_reply.started":"2024-04-14T14:10:43.256918Z"},"trusted":true},"outputs":[],"source":["#evaluate the model\n","def evaluate(model, dataset):\n","    model.eval()\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n","    with torch.no_grad():\n","        accuracies = []\n","        pb = tqdm(dataloader, desc=\"Evaluating...\")\n","        for batch in pb:\n","            encoded_seqs, attention_masks, labels = batch\n","            encoded_seqs = encoded_seqs.to(device)\n","            attention_masks = attention_masks.to(device)\n","            labels = labels.to(device)\n","            output = model(encoded_seqs, attention_masks)\n","            #calculate accuracy\n","            # print(output.round(), labels)\n","            accuracy = (output.round() == labels).sum().item() / labels.numel()\n","            accuracies.append(accuracy)\n","        mean_accuracy = np.mean(accuracies)\n","        print(f\"Mean accuracy: {mean_accuracy}\")\n","    return mean_accuracy\n"]},{"cell_type":"markdown","metadata":{},"source":["## BERT Base Uncased [BERT Freezed]"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:43:56.433977Z","iopub.status.busy":"2024-04-13T14:43:56.433583Z","iopub.status.idle":"2024-04-13T14:43:57.049546Z","shell.execute_reply":"2024-04-13T14:43:57.048648Z","shell.execute_reply.started":"2024-04-13T14:43:56.433947Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing BERT_Base_Multilabel...\n","Initialized.\n"]}],"source":["cls_model = BERT_Base_Multilabel(num_labels).to(device)\n","full_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, tokenizer)\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:43:57.987204Z","iopub.status.busy":"2024-04-13T14:43:57.986483Z","iopub.status.idle":"2024-04-13T14:43:57.992609Z","shell.execute_reply":"2024-04-13T14:43:57.991624Z","shell.execute_reply.started":"2024-04-13T14:43:57.987155Z"},"trusted":true},"outputs":[],"source":["#freeze all BERT params\n","for param in cls_model.bert.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":48,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-13T14:44:06.636798Z","iopub.status.busy":"2024-04-13T14:44:06.636072Z","iopub.status.idle":"2024-04-13T14:44:06.800267Z","shell.execute_reply":"2024-04-13T14:44:06.798958Z","shell.execute_reply.started":"2024-04-13T14:44:06.636768Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n","Epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1250 [00:00<?, ?it/s]\n"]},{"ename":"TypeError","evalue":"'NoneType' object is not callable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#continue training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcls_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[40], line 68\u001b[0m, in \u001b[0;36mBERT_Base_Multilabel.fit\u001b[0;34m(self, epochs, batch_size, lr, dataset, epochs_done)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# print(\"here1\")\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     71\u001b[0m     encoded_seqs, attention_masks, labels \u001b[38;5;241m=\u001b[39m batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","Cell \u001b[0;32mIn[40], line 14\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 14\u001b[0m     char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     encoded_seq \u001b[38;5;241m=\u001b[39m char[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m char[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"]}],"source":["#continue training\n","cls_model.fit(1, 32, 1e-4, train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-13T14:32:52.788941Z","iopub.status.idle":"2024-04-13T14:32:52.789243Z","shell.execute_reply":"2024-04-13T14:32:52.789105Z","shell.execute_reply.started":"2024-04-13T14:32:52.789093Z"},"trusted":true},"outputs":[],"source":["# evaluate(cls_model, test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## BERT Base Cased - [Bert Freezed]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:11:03.721992Z","iopub.status.busy":"2024-04-14T14:11:03.721641Z","iopub.status.idle":"2024-04-14T14:11:04.395228Z","shell.execute_reply":"2024-04-14T14:11:04.394269Z","shell.execute_reply.started":"2024-04-14T14:11:03.721966Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing BERT_BaseCased_Multilabel...\n","Initialized.\n","BERT_BaseCased_Multilabel(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls_head): Sequential(\n","    (0): Linear(in_features=768, out_features=1024, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=1024, out_features=14, bias=True)\n","    (3): Sigmoid()\n","  )\n","  (loss_fn): BCELoss()\n",")\n"]}],"source":["# Trying BERT_cased for 1 epoch\n","\n","cls_bertcased = BERT_BaseCased_Multilabel(num_labels).to(device)\n","full_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, cls_bertcased.tokenizer)\n","print(cls_bertcased)\n","#freeze all BERT params\n","for params in cls_bertcased.bert.parameters():\n","    params.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:11:09.100374Z","iopub.status.busy":"2024-04-14T14:11:09.100001Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["Loss: 0.4240747392177582:  96%|█████████▋| 1205/1250 [17:04<00:38,  1.17it/s] "]}],"source":["cls_bertcased.fit(1, 32, 1e-4, train_dataset)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:35:42.816665Z","iopub.status.busy":"2024-04-14T14:35:42.815485Z","iopub.status.idle":"2024-04-14T14:38:07.875243Z","shell.execute_reply":"2024-04-14T14:38:07.874276Z","shell.execute_reply.started":"2024-04-14T14:35:42.816615Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating...: 100%|██████████| 5000/5000 [02:25<00:00, 34.47it/s]"]},{"name":"stdout","output_type":"stream","text":["Mean accuracy: 0.8383\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test_accuracy = evaluate(cls_bertcased, test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Uncased BERTBase [Last Encoder Layer Unfrozen]"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:56:16.663166Z","iopub.status.busy":"2024-04-14T14:56:16.662185Z","iopub.status.idle":"2024-04-14T14:56:17.507207Z","shell.execute_reply":"2024-04-14T14:56:17.506244Z","shell.execute_reply.started":"2024-04-14T14:56:16.663123Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing BERT_BaseCased_Multilabel...\n","Initialized.\n","Last 1 Encoder Layer Unfreezed\n"]}],"source":["base_unfreeze1 =  BERT_BaseCased_Multilabel(num_labels).to(device)\n","full_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, base_unfreeze1.tokenizer)\n","\n","cnt = 0\n","for param in base_unfreeze1.bert.parameters():\n","    param.requires_grad = False\n","    \n","for layer in base_unfreeze1.bert.encoder.layer:\n","    cnt += 1\n","    if cnt >= 12:\n","        for param in layer.parameters():\n","            param.required_grad = True\n","\n","print(\"Last 1 Encoder Layer Unfreezed\")\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:56:24.616450Z","iopub.status.busy":"2024-04-14T14:56:24.615689Z","iopub.status.idle":"2024-04-14T15:14:06.792005Z","shell.execute_reply":"2024-04-14T15:14:06.791105Z","shell.execute_reply.started":"2024-04-14T14:56:24.616418Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["Loss: 0.4362364709377289: 100%|██████████| 1250/1250 [17:41<00:00,  1.18it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 completed. Training Loss: 0.4362364709377289\n"]}],"source":["base_unfreeze1.fit(1, 32, 1e-4, train_dataset)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T15:44:32.865728Z","iopub.status.busy":"2024-04-14T15:44:32.864924Z","iopub.status.idle":"2024-04-14T15:46:59.752308Z","shell.execute_reply":"2024-04-14T15:46:59.751315Z","shell.execute_reply.started":"2024-04-14T15:44:32.865695Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating...: 100%|██████████| 5000/5000 [02:26<00:00, 34.04it/s]"]},{"name":"stdout","output_type":"stream","text":["Mean accuracy: 0.8380285714285713\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test_accuracy_base_unfreeze1 = evaluate(base_unfreeze1, test_dataset)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T15:59:48.611046Z","iopub.status.busy":"2024-04-14T15:59:48.610592Z","iopub.status.idle":"2024-04-14T15:59:48.894503Z","shell.execute_reply":"2024-04-14T15:59:48.893724Z","shell.execute_reply.started":"2024-04-14T15:59:48.611014Z"},"trusted":true},"outputs":[],"source":["# Lower the learning rate and train for some more time\n","base_unfreeze1.load(\"/kaggle/working/bertbasecased_1_2024-04-14_15:14:06.pt\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T15:59:52.129651Z","iopub.status.busy":"2024-04-14T15:59:52.128845Z","iopub.status.idle":"2024-04-14T16:17:37.135815Z","shell.execute_reply":"2024-04-14T16:17:37.134798Z","shell.execute_reply.started":"2024-04-14T15:59:52.129619Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n"]},{"name":"stderr","output_type":"stream","text":["Loss: 0.3768368065357208: 100%|██████████| 1250/1250 [17:44<00:00,  1.17it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 completed. Training Loss: 0.3768368065357208\n"]}],"source":["base_unfreeze1.fit(2, 32, 2e-5, train_dataset, epochs_done=1)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:24:33.370051Z","iopub.status.busy":"2024-04-14T16:24:33.369652Z","iopub.status.idle":"2024-04-14T16:26:58.606919Z","shell.execute_reply":"2024-04-14T16:26:58.606121Z","shell.execute_reply.started":"2024-04-14T16:24:33.370022Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating...: 100%|██████████| 5000/5000 [02:25<00:00, 34.43it/s]"]},{"name":"stdout","output_type":"stream","text":["Mean accuracy: 0.844\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["accuracy_3 = evaluate(base_unfreeze1, test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Unfreezing Embedding Layer"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:32:07.832174Z","iopub.status.busy":"2024-04-14T16:32:07.831479Z","iopub.status.idle":"2024-04-14T16:32:08.501355Z","shell.execute_reply":"2024-04-14T16:32:08.500393Z","shell.execute_reply.started":"2024-04-14T16:32:07.832140Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing BERT_Base_Multilabel...\n","Initialized.\n","Last 1 Encoder Layer Unfreezed\n","Embedding Layer Unfreezed\n"]}],"source":["\n","uncased_uf_emb_enc = BERT_Base_Multilabel(num_labels).to(device)\n","full_dataset, train_dataset, val_dataset, test_dataset = construct_dataset(texts, target, uncased_uf_emb_enc.tokenizer)\n","\n","cnt = 0\n","for param in uncased_uf_emb_enc.bert.parameters():\n","    param.requires_grad = False\n","    \n","for layer in uncased_uf_emb_enc.bert.encoder.layer:\n","    cnt += 1\n","    if cnt >= 12:\n","        for param in layer.parameters():\n","            param.required_grad = True\n","\n","print(\"Last 1 Encoder Layer Unfreezed\")\n","for param in uncased_uf_emb_enc.bert.embeddings.parameters():\n","    param.requires_grad = True\n","\n","print(\"Embedding Layer Unfreezed\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:33:00.437693Z","iopub.status.busy":"2024-04-14T16:33:00.437022Z","iopub.status.idle":"2024-04-14T17:04:50.044143Z","shell.execute_reply":"2024-04-14T17:04:50.043294Z","shell.execute_reply.started":"2024-04-14T16:33:00.437659Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["Loss: 0.27036356925964355: 100%|██████████| 2500/2500 [31:48<00:00,  1.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 completed. Training Loss: 0.27036356925964355\n"]}],"source":["uncased_uf_emb_enc.fit(1,16,1e-4, train_dataset)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:12:52.970971Z","iopub.status.busy":"2024-04-14T17:12:52.970557Z","iopub.status.idle":"2024-04-14T17:15:26.766711Z","shell.execute_reply":"2024-04-14T17:15:26.765741Z","shell.execute_reply.started":"2024-04-14T17:12:52.970941Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating...: 100%|██████████| 5000/5000 [02:33<00:00, 32.51it/s]"]},{"name":"stdout","output_type":"stream","text":["Mean accuracy: 0.8775142857142859\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["0.8775142857142859"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["evaluate(uncased_uf_emb_enc, test_dataset)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:29:10.812506Z","iopub.status.busy":"2024-04-14T17:29:10.812123Z","iopub.status.idle":"2024-04-14T17:29:11.084754Z","shell.execute_reply":"2024-04-14T17:29:11.083841Z","shell.execute_reply.started":"2024-04-14T17:29:10.812476Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Last 2 Encoder Layer Unfreezed\n","Embedding Layer Unfreezed\n"]}],"source":["# unfreeze one more layer and do 2 more epochs\n","uncased_uf_emb_enc.load(\"/kaggle/working/bertbaseuncased_1_2024-04-14_17:04:49.pt\")\n","cnt = 0\n","for param in uncased_uf_emb_enc.bert.parameters():\n","    param.requires_grad = False\n","    \n","for layer in uncased_uf_emb_enc.bert.encoder.layer:\n","    cnt += 1\n","    if cnt >= 11:\n","        for param in layer.parameters():\n","            param.required_grad = True\n","\n","print(\"Last 2 Encoder Layer Unfreezed\")\n","for param in uncased_uf_emb_enc.bert.embeddings.parameters():\n","    param.requires_grad = True\n","\n","print(\"Embedding Layer Unfreezed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:30:58.339746Z","iopub.status.busy":"2024-04-14T17:30:58.339124Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n"]},{"name":"stderr","output_type":"stream","text":["Loss: 0.2806262969970703:   6%|▌         | 154/2500 [01:57<29:33,  1.32it/s] "]}],"source":["uncased_uf_emb_enc.fit(3,16,4e-5, train_dataset, epochs_done=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(uncased_uf_emb_enc, test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Adding Domain Specific Tokens to the vocabulary and fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4788727,"sourceId":8107454,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
